name: GPU CI Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      gpu_type:
        description: 'GPU type to test on'
        required: false
        default: 'T4'
        type: choice
        options:
          - T4
          - V100
          - A100

jobs:
  build:
    name: Build and Test on GPU
    runs-on: [self-hosted, linux, gpu, "${{ github.event.inputs.gpu_type || 'T4' }}"]
    
    env:
      PYTHON_VERSION: "3.9"
      CUDA_VERSION: "11.8"
      JAX_PLATFORM_NAME: "gpu"
      MCTX_ENABLE_T4_OPTIMIZATIONS: "1"
      XLA_PYTHON_CLIENT_ALLOCATOR: "platform"
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Check GPU availability
        run: |
          nvidia-smi
          echo "GPU count: $(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)"
          echo "GPU type: $(nvidia-smi --query-gpu=name --format=csv,noheader | head -n 1)"
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements/requirements.txt
          # Install JAX with CUDA support
          pip install jax[cuda11_pip]==0.4.16 jaxlib==0.4.16 --find-links https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
          # Install test dependencies
          pip install pytest pytest-cov pytest-benchmark
          # Install the package in development mode
          pip install -e .
      
      - name: Verify JAX GPU configuration
        run: |
          python -c "import jax; print('JAX devices:', jax.devices()); print('Default backend:', jax.default_backend())"
      
      - name: Run unit tests on GPU
        run: |
          pytest tests/ -v --cov=mctx --cov-report=xml
      
      - name: Run GPU-specific tests
        run: |
          pytest tests/test_gpu_*.py -v
      
      - name: Run T4 optimization benchmarks
        run: |
          python examples/t4_optimization_demo.py --benchmark --output-json=benchmark_results.json
      
      - name: Upload code coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: false
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark_results.json
  
  docker-build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: build
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Build and cache Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./docker/Dockerfile.nvidia.optimized
          push: false
          tags: mctx-nvidia:test
          cache-from: type=gha
          cache-to: type=gha,mode=max
          load: true
      
      - name: Test Docker image
        run: |
          docker run --rm mctx-nvidia:test python -c "import mctx; print('MCTX version:', mctx.__version__)"
  
  integration-test:
    name: GPU Integration Tests
    runs-on: [self-hosted, linux, gpu, "${{ github.event.inputs.gpu_type || 'T4' }}"]
    needs: docker-build
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Docker Compose Up
        run: |
          docker-compose -f docker-compose.nvidia.yml up -d mctx-nvidia
      
      - name: Run integration tests
        run: |
          # Wait for services to be ready
          sleep 10
          
          # Test API endpoint
          curl -f http://localhost:8000/health || exit 1
          
          # Run performance test
          docker exec mctx-nvidia python examples/t4_optimization_demo.py --performance-test
      
      - name: Collect and publish metrics
        run: |
          # Extract performance metrics
          docker exec mctx-nvidia python -c "import json; from mctx.monitoring.metrics import SearchMetrics; metrics = SearchMetrics(); print(json.dumps(metrics.as_dict()))" > performance_metrics.json
      
      - name: Upload performance metrics
        uses: actions/upload-artifact@v3
        with:
          name: performance-metrics
          path: performance_metrics.json
      
      - name: Docker Compose Down
        run: |
          docker-compose -f docker-compose.nvidia.yml down