# MCTX NVIDIA LaunchPad Blueprint Docker Compose
# Optimized for NVIDIA T4 GPU deployment

services:
  # API service - Main backend API with MCTX integration and T4 optimizations
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.nvidia
    image: mctx-nvidia-api:latest
    container_name: mctx-nvidia-api
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      - ./mctx:/app/mctx
      - ./api:/app/api
      - ./logs:/app/logs
      - ./examples:/app/examples
    environment:
      # Core settings
      - DEBUG=${DEBUG:-False}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
      
      # GPU settings
      - MCTX_ENABLE_T4_OPTIMIZATIONS=1
      - MCTX_ENABLE_TENSOR_CORE_SIMULATION=1
      
      # Performance settings
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-128}
      - MAX_NUM_SIMULATIONS=${MAX_NUM_SIMULATIONS:-1000}
      - NUM_THREADS=${NUM_THREADS:-8}
      
      # API settings
      - CORS_ORIGINS=http://localhost:3000,http://localhost:8050,http://localhost:8051,http://localhost:8080
      - API_SECRET_KEY=${API_SECRET_KEY:-default_development_key_replace_in_production}
      - API_KEY_REQUIRED=${API_KEY_REQUIRED:-false}
      - API_KEY=${API_KEY:-test_api_key}
      
      # Redis settings
      - REDIS_ENABLED=${REDIS_ENABLED:-true}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
    restart: unless-stopped
    depends_on:
      - redis
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
    command: ["api", "--reload", "--log-level", "info"]
  
  # Visualization service - Interactive MCTS tree visualization
  visualization:
    build:
      context: .
      dockerfile: docker/Dockerfile.nvidia
    image: mctx-nvidia-api:latest
    container_name: mctx-visualization
    ports:
      - "${VISUALIZATION_PORT:-8050}:8050"
    volumes:
      - ./mctx:/app/mctx
      - ./examples:/app/examples
    environment:
      - DEBUG=${DEBUG:-False}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
      - MCTX_ENABLE_T4_OPTIMIZATIONS=1
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8050/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    command: ["visualize", "--port", "8050"]
  
  # Monitoring service - Advanced performance monitoring dashboard
  monitoring:
    build:
      context: .
      dockerfile: docker/Dockerfile.nvidia
    image: mctx-nvidia-api:latest
    container_name: mctx-monitoring
    ports:
      - "${MONITORING_PORT:-8051}:8051"
    volumes:
      - ./mctx:/app/mctx
      - ./examples:/app/examples
    environment:
      - DEBUG=${DEBUG:-False}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
      - MCTX_ENABLE_T4_OPTIMIZATIONS=1
    restart: unless-stopped
    depends_on:
      - api
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8051/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    command: ["python", "examples/monitoring_demo.py", "--port", "8051", "--save-visualizations"]

  # Redis for caching and rate limiting
  redis:
    image: redis:alpine
    container_name: mctx-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "3"
  
  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: mctx-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./config/prometheus/gpu.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    depends_on:
      - api
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
  
  # Grafana for visualization dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: mctx-grafana
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
      - GF_LOG_MODE=console file
      - GF_LOG_LEVEL=info
    volumes:
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Documentation server
  docs:
    image: nginx:alpine
    container_name: mctx-docs
    volumes:
      - ./docs:/usr/share/nginx/html
    ports:
      - "${DOCS_PORT:-8080}:80"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "nginx -t || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "2"

volumes:
  redis_data:
    name: mctx_nvidia_redis_data
  prometheus_data:
    name: mctx_nvidia_prometheus_data
  grafana_data:
    name: mctx_nvidia_grafana_data

networks:
  default:
    name: mctx-nvidia-network